{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorflow1.0 에 있는 wide_n_deep_tutorial.py 를 분석해 보았습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "우선은 import 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import sys\n",
    "import tempfile\n",
    "\n",
    "from six.moves import urllib\n",
    "\n",
    "import pandas as pd\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "원 소스에 보면 tempfile이 있으나 여기서는 사용하지 않을 것 입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "소스를 처음부터가 아닌 흐름을 파악할 수 있게 순서를 변경해서 보도록 하겠습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## argument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "  parser = argparse.ArgumentParser()\n",
    "  parser.register(\"type\", \"bool\", lambda v: v.lower() == \"true\")\n",
    "  parser.add_argument(\n",
    "      \"--model_dir\",\n",
    "      type=str,\n",
    "      default=\"./model\",\n",
    "      help=\"Base directory for output models.\"\n",
    "  )\n",
    "  parser.add_argument(\n",
    "      \"--model_type\",\n",
    "      type=str,\n",
    "      default=\"wide_n_deep\",\n",
    "      help=\"Valid model types: {'wide', 'deep', 'wide_n_deep'}.\"\n",
    "  )\n",
    "  parser.add_argument(\n",
    "      \"--train_steps\",\n",
    "      type=int,\n",
    "      default=200,\n",
    "      help=\"Number of training steps.\"\n",
    "  )\n",
    "  parser.add_argument(\n",
    "      \"--train_data\",\n",
    "      type=str,\n",
    "      default=\"./data/training_data.csv\",\n",
    "      help=\"Path to the training data.\"\n",
    "  )\n",
    "  parser.add_argument(\n",
    "      \"--test_data\",\n",
    "      type=str,\n",
    "      default=\"./data/test_data.csv\",\n",
    "      help=\"Path to the test data.\"\n",
    "  )\n",
    "  FLAGS, unparsed = parser.parse_known_args()\n",
    "  tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* parser에 값들이 추가 되었고, 그 값들이 FLAGS에 들어간 점을 눈여겨 보세요.\n",
    "* FLAG는 환경설정 값으로 계속해서 사용됩니다.\n",
    "```\n",
    "  parser.add_argument(\n",
    "      \"--model_dir\",\n",
    "      type=str,\n",
    "      default=\"./model\",\n",
    "      help=\"Base directory for output models.\"\n",
    "  )\n",
    "```  \n",
    "* argument로 --model_dir=디렉토리 이렇게 들어오다면 값이 '디렉토리'로 들어가고, argument가 없다면 default 값이 사용되는 것 입니다.\n",
    "* 꺼내 쓸 때는 FLAG.model_dir 로 쓸 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "FLAGS = None\n",
    "\n",
    "\n",
    "def main(_):\n",
    "  train_and_eval(FLAGS.model_dir, FLAGS.model_type, FLAGS.train_steps,\n",
    "                 FLAGS.train_data, FLAGS.test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* argument 에서 설정된 값이 train_and_eval 로 전달됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train_and_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_and_eval(model_dir, model_type, train_steps, train_data, test_data):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"Train and evaluate the model.\"\"\"\n",
    "  train_file_name, test_file_name = maybe_download(train_data, test_data)\n",
    "  df_train = pd.read_csv(\n",
    "      tf.gfile.Open(train_file_name),\n",
    "      names=COLUMNS,\n",
    "      skipinitialspace=True,\n",
    "      engine=\"python\")\n",
    "  df_test = pd.read_csv(\n",
    "      tf.gfile.Open(test_file_name),\n",
    "      names=COLUMNS,\n",
    "      skipinitialspace=True,\n",
    "      skiprows=1,\n",
    "      engine=\"python\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* maybe_download 는 데이터를 다운 받는 함수이고 이후에 나타납니다.\n",
    "* pd는 panda라이브러리입니다. pd로 csv파일을 읽습니다.\n",
    "* COLUMNS 는 문자들이 들어있는 배열입니다.\n",
    "* csv파일에 comma(,)로 구분된 각 컬럼에 순서 대로 이름을 매칭시킵니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# remove NaN elements\n",
    "  df_train = df_train.dropna(how='any', axis=0)\n",
    "  df_test = df_test.dropna(how='any', axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [dropna](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.dropna.html#) 에 전달된 값을 보면 axis가 0이거나 1일 수 있는데, 0이면 row단위로 없는 값을 제거 한 뒤 리턴하고, 1이면 column이 빈 것을 제거하로 리턴합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_train[LABEL_COLUMN] = (\n",
    "      df_train[\"income_bracket\"].apply(lambda x: \">50K\" in x)).astype(int)\n",
    "\n",
    "  df_test[LABEL_COLUMN] = (\n",
    "      df_test[\"income_bracket\"].apply(lambda x: \">50K\" in x)).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* LABEL_COLUMN 에는 y값으로 사용할 컬럼명이 들어가 있습니다.\n",
    "* panda (pd) 에서 리턴해준 df_train에서 이름이 \"income_bracket\" 것을 찾아 \">50K\"가 있으면 1 없으면 0으로 값을 넣어 줍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_dir = tempfile.mkdtemp() if not model_dir else model_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 조건문입니다. \n",
    "* 만약 model_dir이 None이거나 False이면 tempfile.mkdtemp()를 호출하여 model_dir에 넣어주고, 그렇지 않다면 model_dir을 넣어 줍니다.\n",
    "* 즉, model_dir이 None이거나 False인지를 체크해서 model_dir의 값을 넣어 줍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "m = build_estimator(model_dir, model_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 모델을 만들어 줍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "m.fit(input_fn=lambda: input_fn(df_train), steps=train_steps)\n",
    "  results = m.evaluate(input_fn=lambda: input_fn(df_test), steps=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* m.fit 모델을 학습 시킵니다. input_fn함수에 df_train을 넣어줍니다.\n",
    "* m.evaluate는 학습된 모델을 평가합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "results = m.evaluate(input_fn=lambda: input_fn(df_test), steps=1)\n",
    "  for key in sorted(results):\n",
    "    print(\"%s: %s\" % (key, results[key]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 출력 값은 아래처럼 나왔습니다.\n",
    "* accuracy: 0.818193\n",
    "* accuracy/baseline_label_mean: 0.236226\n",
    "* accuracy/threshold_0.500000_mean: 0.818193\n",
    "* auc: 0.861029\n",
    "* global_step: 202\n",
    "* labels/actual_label_mean: 0.236226\n",
    "* labels/prediction_mean: 0.32576\n",
    "* loss: 1.24135\n",
    "* precision/positive_threshold_0.500000_mean: 0.611251\n",
    "* recall/positive_threshold_0.500000_mean: 0.632865"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## build_estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_estimator(model_dir, model_type):\n",
    "  \"\"\"Build an estimator.\"\"\"\n",
    "  # Sparse base columns.\n",
    "\n",
    "  '''\n",
    "  몇 개 안되는 값일 때... 즉 몇 가지 케이스로 정해진 경우\n",
    "  '''\n",
    "  gender = tf.contrib.layers.sparse_column_with_keys(column_name=\"gender\",\n",
    "                                                     keys=[\"female\", \"male\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* sparse column들을 만드는데, 몇 가지 타입이 있습니다.\n",
    "* [sparse_column_with_keys](https://www.tensorflow.org/api_docs/python/tf/contrib/layers/sparse_column_with_keys) 는 값이 몇 가지로 한정되어 있을 때 사용합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "  해쉬 값으로 읽음.\n",
    "  '''\n",
    "  education = tf.contrib.layers.sparse_column_with_hash_bucket(\n",
    "      \"education\", hash_bucket_size=1000)\n",
    "  relationship = tf.contrib.layers.sparse_column_with_hash_bucket(\n",
    "      \"relationship\", hash_bucket_size=100)\n",
    "  workclass = tf.contrib.layers.sparse_column_with_hash_bucket(\n",
    "      \"workclass\", hash_bucket_size=100)\n",
    "  occupation = tf.contrib.layers.sparse_column_with_hash_bucket(\n",
    "      \"occupation\", hash_bucket_size=1000)\n",
    "  native_country = tf.contrib.layers.sparse_column_with_hash_bucket(\n",
    "      \"native_country\", hash_bucket_size=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [sparse_column_with_hash_bucket](https://www.tensorflow.org/api_docs/python/tf/contrib/layers/sparse_column_with_hash_bucket) 는 문자열로 된 것들을 hash 값으로 변경해서 사용합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Continuous base columns.\n",
    "  '''\n",
    "  숫자 형.\n",
    "  '''\n",
    "  age = tf.contrib.layers.real_valued_column(\"age\")\n",
    "  education_num = tf.contrib.layers.real_valued_column(\"education_num\")\n",
    "  capital_gain = tf.contrib.layers.real_valued_column(\"capital_gain\")\n",
    "  capital_loss = tf.contrib.layers.real_valued_column(\"capital_loss\")\n",
    "  hours_per_week = tf.contrib.layers.real_valued_column(\"hours_per_week\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [real_valued_column](https://www.tensorflow.org/api_docs/python/tf/contrib/layers/real_valued_column)는 숫자형 데이터 입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Transformations.\n",
    "  age_buckets = tf.contrib.layers.bucketized_column(age,\n",
    "                                                    boundaries=[\n",
    "                                                        18, 25, 30, 35, 40, 45,\n",
    "                                                        50, 55, 60, 65\n",
    "                                                    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 이 곳에서는 나이를 5살 단위로 만들어 줍니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Wide columns and deep columns.\n",
    "  wide_columns = [gender, native_country, education, occupation, workclass,\n",
    "                  relationship, age_buckets,\n",
    "                  tf.contrib.layers.crossed_column([education, occupation],\n",
    "                                                   hash_bucket_size=int(1e4)),\n",
    "                  tf.contrib.layers.crossed_column(\n",
    "                      [age_buckets, education, occupation],\n",
    "                      hash_bucket_size=int(1e6)),\n",
    "                  tf.contrib.layers.crossed_column([native_country, occupation],\n",
    "                                                   hash_bucket_size=int(1e4))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 위에서 만들어준 각 변수들을 배열에 넣어 줍니다.\n",
    "* crossed_column 컬럼이 무엇인지 조사 필요 ******************************************************"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "deep_columns = [\n",
    "      tf.contrib.layers.embedding_column(workclass, dimension=8),\n",
    "      tf.contrib.layers.embedding_column(education, dimension=8),\n",
    "      tf.contrib.layers.embedding_column(gender, dimension=8),\n",
    "      tf.contrib.layers.embedding_column(relationship, dimension=8),\n",
    "      tf.contrib.layers.embedding_column(native_country,\n",
    "                                         dimension=8),\n",
    "      tf.contrib.layers.embedding_column(occupation, dimension=8),\n",
    "      age,\n",
    "      education_num,\n",
    "      capital_gain,\n",
    "      capital_loss,\n",
    "      hours_per_week,\n",
    "  ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* embedding_column 이 무엇인지 조사 필요  ******************************************************"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if model_type == \"wide\":\n",
    "    m = tf.contrib.learn.LinearClassifier(model_dir=model_dir,\n",
    "                                          feature_columns=wide_columns)\n",
    "  elif model_type == \"deep\":\n",
    "    m = tf.contrib.learn.DNNClassifier(model_dir=model_dir,\n",
    "                                       feature_columns=deep_columns,\n",
    "                                       hidden_units=[100, 50])\n",
    "  else:\n",
    "    m = tf.contrib.learn.DNNLinearCombinedClassifier(\n",
    "        model_dir=model_dir,\n",
    "        linear_feature_columns=wide_columns,\n",
    "        dnn_feature_columns=deep_columns,\n",
    "        dnn_hidden_units=[100, 50]\n",
    "        # ,fix_global_step_increment_bug=True\n",
    "    )\n",
    "  return m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 여기서는 model_type이 wide_n_deep 이므로 else: 부분을 타게 됩니다.\n",
    "* DNNLinearCombinedClassifier 가 무엇인지 조사 필요. ******************************************************"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## input_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* input_fn은 train_and_eval에서 모델을 학습 시킬 때 input 값으로 들어가는 람다 함수 입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "continuous_cols = {k: tf.constant(df[k].values) for k in CONTINUOUS_COLUMNS}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* CONTINUOUS_COLUMNS 에 들어있는 String 값을 하나씩 꺼내서 k로 가게 되고 key:value 형식의 데이터로 만들어 continuous_cols에 리턴 됩니다.\n",
    "* 이 곳에는 숫자형 데이터만 들어 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "categorical_cols = {\n",
    "      k: tf.SparseTensor(\n",
    "          indices=[[i, 0] for i in range(df[k].size)],\n",
    "          values=df[k].values,\n",
    "          dense_shape=[df[k].size, 1])\n",
    "      for k in CATEGORICAL_COLUMNS}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}